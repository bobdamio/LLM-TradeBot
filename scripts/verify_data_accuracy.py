#!/usr/bin/env python3
"""
æ•°æ®æµè½¬å‡†ç¡®æ€§éªŒè¯è„šæœ¬
éªŒè¯ä»åŸå§‹æ•°æ®åˆ°å†³ç­–çš„å®Œæ•´æ•°æ®é“¾è·¯
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path

class DataAccuracyChecker:
    def __init__(self, data_dir='data', date='20251220', snapshot_id='snap_1766234252'):
        self.data_dir = Path(data_dir)
        self.date = date
        self.snapshot_id = snapshot_id
        self.symbol = 'BTCUSDT'
        self.timeframe = '5m'
        
    def check_stage1_raw_data(self):
        """Stage 1: éªŒè¯åŸå§‹å¸‚åœºæ•°æ®"""
        print("=" * 80)
        print("Stage 1: åŸå§‹å¸‚åœºæ•°æ®éªŒè¯")
        print("=" * 80)
        
        json_file = self.data_dir / 'data_sync_agent' / self.date / f'market_data_{self.symbol}_{self.timeframe}_20251220_203733.json'
        
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        klines = data['klines']
        print(f"âœ“ Kçº¿æ•°é‡: {len(klines)}")
        print(f"âœ“ äº¤æ˜“å¯¹: {data['metadata']['symbol']}")
        
        # éªŒè¯ä»·æ ¼é€»è¾‘
        errors = 0
        for i, k in enumerate(klines[:10]):
            if k['high'] < max(k['open'], k['close']) or k['low'] > min(k['open'], k['close']):
                print(f"âœ— Kçº¿ {i+1} ä»·æ ¼é€»è¾‘é”™è¯¯")
                errors += 1
        
        if errors == 0:
            print(f"âœ“ ä»·æ ¼é€»è¾‘éªŒè¯é€šè¿‡ï¼ˆå‰10æ ¹ï¼‰")
        
        return klines[-1]['close']  # è¿”å›æœ€åæ”¶ç›˜ä»·
    
    def check_stage2_indicators(self):
        """Stage 2: éªŒè¯æŠ€æœ¯æŒ‡æ ‡è®¡ç®—"""
        print("\n" + "=" * 80)
        print("Stage 2: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—éªŒè¯")
        print("=" * 80)
        
        parquet_file = self.data_dir / 'quant_analyst_agent' / 'indicators' / self.date / f'indicators_{self.symbol}_{self.timeframe}_20251220_203733_{self.snapshot_id}.parquet'
        
        df = pd.read_parquet(parquet_file)
        print(f"âœ“ æ•°æ®ç»´åº¦: {df.shape}")
        print(f"âœ“ æŒ‡æ ‡åˆ—æ•°: {len(df.columns)}")
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        last_row = df.iloc[-1]
        print(f"âœ“ æœ€åæ”¶ç›˜ä»·: {last_row['close']:.2f}")
        print(f"âœ“ EMA12: {last_row['ema_12']:.2f}")
        print(f"âœ“ EMA26: {last_row['ema_26']:.2f}")
        print(f"âœ“ RSI: {last_row['rsi']:.2f}")
        print(f"âœ“ MACD: {last_row['macd']:.2f}")
        
        # æ£€æŸ¥ NaN å€¼ï¼ˆWarm-up æœŸé™¤å¤–ï¼‰
        valid_data = df[df['is_valid'] == True]
        nan_in_valid = valid_data.isna().sum().sum()
        print(f"âœ“ æœ‰æ•ˆæ•°æ®ä¸­ NaN æ•°: {nan_in_valid}")
        
        return last_row['close'], last_row['rsi'], last_row['ema_12'], last_row['ema_26']
    
    def check_stage3_features(self):
        """Stage 3: éªŒè¯ç‰¹å¾æå–"""
        print("\n" + "=" * 80)
        print("Stage 3: ç‰¹å¾æå–éªŒè¯")
        print("=" * 80)
        
        parquet_file = self.data_dir / 'quant_analyst_agent' / 'features' / self.date / f'features_{self.symbol}_{self.timeframe}_20251220_203733_{self.snapshot_id}_v1.parquet'
        
        df = pd.read_parquet(parquet_file)
        print(f"âœ“ ç‰¹å¾ç»´åº¦: {df.shape}")
        print(f"âœ“ ç‰¹å¾æ•°é‡: {len(df.columns)}")
        
        last_row = df.iloc[-1]
        print(f"âœ“ æ”¶ç›˜ä»·: {last_row['close']:.2f}")
        print(f"âœ“ æ”¶ç›Šç‡: {last_row['return_pct']:.4f}%")
        print(f"âœ“ MACDç™¾åˆ†æ¯”: {last_row['macd_pct']:.4f}%")
        
        # æ£€æŸ¥ Inf å€¼
        inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
        print(f"âœ“ Inf å€¼æ•°é‡: {inf_count}")
        
        return last_row['close']
    
    def check_stage4_quant_analysis(self):
        """Stage 4: éªŒè¯é‡åŒ–åˆ†æ"""
        print("\n" + "=" * 80)
        print("Stage 4: é‡åŒ–åˆ†æä¸Šä¸‹æ–‡éªŒè¯")
        print("=" * 80)
        
        json_file = self.data_dir / 'quant_analyst_agent' / 'context' / self.date / f'context_{self.symbol}_quant_analysis_20251220_203733_{self.snapshot_id}.json'
        
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        print(f"âœ“ ä¿¡å·æºæ•°é‡: {len(data)}")
        
        # éªŒè¯è¶‹åŠ¿ä¿¡å·
        trend_scores = []
        for period in ['5m', '15m', '1h']:
            key = f'trend_{period}'
            if key in data:
                score = data[key]['score']
                signal = data[key]['signal']
                trend_scores.append(score)
                print(f"âœ“ {period} è¶‹åŠ¿: {signal} (å¾—åˆ†: {score})")
        
        # éªŒè¯éœ‡è¡ä¿¡å·
        for period in ['5m', '15m', '1h']:
            key = f'oscillator_{period}'
            if key in data:
                score = data[key]['score']
                signal = data[key]['signal']
                print(f"âœ“ {period} éœ‡è¡: {signal} (å¾—åˆ†: {score})")
        
        return sum(trend_scores)
    
    def check_stage5_decision(self):
        """Stage 5: éªŒè¯å†³ç­–ç»“æœ"""
        print("\n" + "=" * 80)
        print("Stage 5: å†³ç­–ç»“æœéªŒè¯")
        print("=" * 80)
        
        json_file = self.data_dir / 'decision_core_agent' / 'decisions' / self.date / f'decision_{self.symbol}_20251220_203733_{self.snapshot_id}.json'
        
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        print(f"âœ“ å†³ç­–åŠ¨ä½œ: {data['action']}")
        print(f"âœ“ ç½®ä¿¡åº¦: {data['confidence']:.4f} ({data['confidence']*100:.2f}%)")
        print(f"âœ“ åŠ æƒå¾—åˆ†: {data['weighted_score']}")
        print(f"âœ“ å¤šå‘¨æœŸå¯¹é½: {data['multi_period_aligned']}")
        
        # éªŒè¯åŠ æƒå¾—åˆ†è®¡ç®—
        calculated_score = sum(data['vote_details'].values())
        print(f"\nåŠ æƒå¾—åˆ†éªŒè¯:")
        print(f"  è®°å½•å€¼: {data['weighted_score']}")
        print(f"  è®¡ç®—å€¼: {calculated_score:.2f}")
        diff = abs(data['weighted_score'] - calculated_score)
        if diff < 0.01:
            print(f"  âœ“ è®¡ç®—æ­£ç¡® (å·®å¼‚: {diff:.6f})")
        else:
            print(f"  âœ— è®¡ç®—æœ‰è¯¯å·® (å·®å¼‚: {diff:.6f})")
        
        return data['action'], data['weighted_score']
    
    def check_data_consistency(self):
        """è·¨é˜¶æ®µæ•°æ®ä¸€è‡´æ€§éªŒè¯"""
        print("\n" + "=" * 80)
        print("è·¨é˜¶æ®µæ•°æ®ä¸€è‡´æ€§éªŒè¯")
        print("=" * 80)
        
        # è¯»å–å„é˜¶æ®µæœ€åä¸€è¡Œæ•°æ®
        json_file = self.data_dir / 'data_sync_agent' / self.date / f'market_data_{self.symbol}_{self.timeframe}_20251220_203733.json'
        with open(json_file, 'r') as f:
            raw_data = json.load(f)
        raw_close = raw_data['klines'][-1]['close']
        
        indicators_file = self.data_dir / 'quant_analyst_agent' / 'indicators' / self.date / f'indicators_{self.symbol}_{self.timeframe}_20251220_203733_{self.snapshot_id}.parquet'
        df_ind = pd.read_parquet(indicators_file)
        ind_close = df_ind.iloc[-1]['close']
        
        features_file = self.data_dir / 'quant_analyst_agent' / 'features' / self.date / f'features_{self.symbol}_{self.timeframe}_20251220_203733_{self.snapshot_id}_v1.parquet'
        df_feat = pd.read_parquet(features_file)
        feat_close = df_feat.iloc[-1]['close']
        
        print(f"åŸå§‹æ•°æ®æ”¶ç›˜ä»·: {raw_close:.2f}")
        print(f"æŒ‡æ ‡æ•°æ®æ”¶ç›˜ä»·: {ind_close:.2f}")
        print(f"ç‰¹å¾æ•°æ®æ”¶ç›˜ä»·: {feat_close:.2f}")
        
        if abs(raw_close - ind_close) < 0.01 and abs(ind_close - feat_close) < 0.01:
            print(f"âœ“ æ”¶ç›˜ä»·ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        else:
            print(f"âœ— æ”¶ç›˜ä»·å­˜åœ¨å·®å¼‚")
        
        # éªŒè¯å¿«ç…§ID
        ind_snapshot = df_ind.iloc[-1]['snapshot_id']
        feat_snapshot = df_feat.iloc[-1]['source_snapshot_id']
        
        print(f"\nå¿«ç…§IDä¸€è‡´æ€§:")
        print(f"  æŒ‡æ ‡: {ind_snapshot}")
        print(f"  ç‰¹å¾: {feat_snapshot}")
        if ind_snapshot == feat_snapshot:
            print(f"  âœ“ å¿«ç…§IDä¸€è‡´")
        else:
            print(f"  âœ— å¿«ç…§IDä¸ä¸€è‡´")
    
    def run_all_checks(self):
        """è¿è¡Œæ‰€æœ‰éªŒè¯"""
        print("\n" + "ğŸ” æ•°æ®æµè½¬å‡†ç¡®æ€§å…¨é¢éªŒè¯")
        print("=" * 80 + "\n")
        
        try:
            # Stage 1
            raw_close = self.check_stage1_raw_data()
            
            # Stage 2
            ind_close, rsi, ema12, ema26 = self.check_stage2_indicators()
            
            # Stage 3
            feat_close = self.check_stage3_features()
            
            # Stage 4
            trend_sum = self.check_stage4_quant_analysis()
            
            # Stage 5
            action, weighted_score = self.check_stage5_decision()
            
            # ä¸€è‡´æ€§æ£€æŸ¥
            self.check_data_consistency()
            
            # æ€»ç»“
            print("\n" + "=" * 80)
            print("éªŒè¯æ€»ç»“")
            print("=" * 80)
            print(f"âœ“ æ‰€æœ‰é˜¶æ®µæ•°æ®éªŒè¯å®Œæˆ")
            print(f"âœ“ æ•°æ®æµè½¬é“¾è·¯å®Œæ•´")
            print(f"âœ“ å…³é”®æ•°æ®ä¸€è‡´æ€§è‰¯å¥½")
            print(f"\næœ€ç»ˆå†³ç­–: {action} (åŠ æƒå¾—åˆ†: {weighted_score})")
            
        except Exception as e:
            print(f"\nâœ— éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

if __name__ == '__main__':
    checker = DataAccuracyChecker()
    checker.run_all_checks()
